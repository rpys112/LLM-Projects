# -*- coding: utf-8 -*-
"""RAG.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uvNNCOpH_3FR4rTnQsGTeKVFxIlZRaEp
"""

import pip


pip install langchain-community

pip install langchain transformers sentence-transformers faiss-gpu torch

from langchain.document_loaders import PyPDFLoader

pip install pypdf

loader = PyPDFLoader("/Malawi Congress Party v President of the Republic of Malawi (Judicial Review Cause 34 of 2020) 2021 MWHC 39 (2 June 2021).pdf")
documents = loader.load()
print(len(documents))

from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 1000,
    chunk_overlap  = 200,
    length_function = len,
)
texts = text_splitter.split_documents(documents)

from langchain.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')

db = chroma.Chroma.from_documents(texts, embeddings)

from langchain.vectorstores import chroma

pip install chromadb

db = chroma.Chroma.from_documents(texts, embeddings)

retriever = db.as_retriever(search_type="similarity", search_kwargs={"k":2})

from langchain.llms import HuggingFacePipeline
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from accelerate import disk_offload
import torch
import os
os.environ["CUDA_MODULE_LOADING"]= "LAZY"

model_id = "distilgpt2"
tokenizer = AutoTokenizer.from_pretrained(model_id)

if torch.cuda.is_available():
    device = "cuda"
else:
    device = "cpu"


model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map="auto",
)

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=256,
    temperature=0.7,
    top_p=0.95,
    repetition_penalty=1.15,
    device_map = "auto"
    )

llm = HuggingFacePipeline(pipeline=pipe)

from langchain.chains import RetrievalQA

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True,
)

query = "What is the context of the case?"
result = qa_chain({"query": query})
result["result"]

